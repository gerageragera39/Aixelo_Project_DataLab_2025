{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "aZXpTZKgBjFW"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.dpi'] = 300"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IGSKnVQqV8YV"
   },
   "source": [
    "df_45 = pd.read_csv('stoich45_fingerprints.csv')\n",
    "df_120 = pd.read_csv('stoich120_fingerprints.csv')\n",
    "df_sine = pd.read_csv('sine_matrix_fingerprints.csv')\n",
    "df_qmof_refcodes = pd.read_csv('qmof-refcodes.csv')\n",
    "df_ofm_fp = pd.read_csv('ofm_fingerprints.csv')\n",
    "\n",
    "df_bandgaps = pd.read_csv('qmof-bandgaps.csv')\n",
    "df_bandgaps = df_bandgaps.rename(columns={'refcode': 'MOF'})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Run this for issues with google collab (i.e MOF not exisiting in the Data frame)"
   ],
   "metadata": {
    "id": "CKMrmKOYWsV8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "if 'MOF' in df_120.columns:\n",
    "    columns = ['MOF'] + [col for col in df_120.columns if col != 'MOF']\n",
    "    df_120 = df_120[columns]\n",
    "else:\n",
    "    raise KeyError(\"The column 'MOF' does not exist in the DataFrame.\")\n",
    "\n",
    "if 'MOF' in df_45.columns:\n",
    "    columns = ['MOF'] + [col for col in df_120.columns if col != 'MOF']\n",
    "    df_120 = df_120[columns]\n",
    "else:\n",
    "    raise KeyError(\"The column 'MOF' does not exist in the DataFrame.\")\n",
    "\n",
    "if 'MOF' in df_bandgaps.columns:\n",
    "    columns = ['MOF'] + [col for col in df_120.columns if col != 'MOF']\n",
    "    df_120 = df_120[columns]\n",
    "else:\n",
    "    raise KeyError(\"The column 'MOF' does not exist in the DataFrame.\")"
   ],
   "metadata": {
    "id": "kLZdps-qWrqG"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "NbV-MKJBRdlZ",
    "outputId": "33175f3a-eaab-4665-a6ba-3b21e18e67cd"
   },
   "source": [
    "#check difference in between 45 and 120 dataset\n",
    "# Extract the column names (features)\n",
    "features_45 = set(df_45.columns)\n",
    "features_120 = set(df_120.columns)\n",
    "\n",
    "# Find features in df1 that are not in df2\n",
    "unique_features_45 = features_45 - features_120\n",
    "\n",
    "# Print the unique features\n",
    "print(\"Features in df1 that are not in df2:\")\n",
    "print(unique_features_45)\n",
    "print(len(unique_features_45))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0a48Ul-gRlXh"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIG5WiaZc0EN"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "**Exploring the 45 fingerprints**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "S_1LSIANZ-gt",
    "outputId": "a1a02196-64cd-464f-9cc9-9e1bc17ac1f9"
   },
   "source": [
    "#df_45.head(10)\n",
    "#display all the columns, all of the 45 features\n",
    "pd.options.display.max_columns = None\n",
    "df_45.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "cPy3yyl8afVT",
    "outputId": "e8f650c6-b4ad-41f2-a0e9-48579c56578c"
   },
   "source": [
    "df_45.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9dxVsJe2uBWD",
    "outputId": "fc466638-4512-4ef0-cff3-004825f90700"
   },
   "source": [
    "df_45.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "N46HTU_UbFwV",
    "outputId": "ae71f3ca-c319-4e33-8c29-9b9c470004c1"
   },
   "source": [
    "#Plotting histograms of each of the 45 fingerprints.\n",
    "\n",
    "numeric_cols = df_45.select_dtypes(include='number').columns\n",
    "\n",
    "num_cols = 5\n",
    "num_rows = (len(numeric_cols) + num_cols - 1) // num_cols\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (col, ax) in enumerate(zip(numeric_cols, axes)):\n",
    "    df_45[col].hist(ax=ax, bins=20)\n",
    "    ax.set_title(col)\n",
    "    ax.set_xlabel('Values')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "for j in range(len(numeric_cols), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "dN6aWWgYa1Iw",
    "outputId": "11d8502a-bfd1-495a-ffe4-632e3a6b15cc"
   },
   "source": [
    "#Plotting Box plots for each 45 features\n",
    "numerical_columns = df_45.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Plot box plots for numerical features in a 9 by 5 grid\n",
    "plt.figure(figsize=(20, 30))  # Adjust figure size for 9 by 5 grid\n",
    "for i, feature in enumerate(numerical_columns, start=1):\n",
    "    plt.subplot(9, 5, i)\n",
    "    plt.boxplot(df_45[feature])\n",
    "    plt.title(feature)\n",
    "    plt.grid(True)\n",
    "    plt.xticks([])  # Remove x-axis labels to save space\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLolDeTC4D_j"
   },
   "source": [
    "**Exploring the target variable - BG_PBE (Band gaps)**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "vwxUZrSOcZtB",
    "outputId": "45d3097d-2f8e-4404-bc48-f52f72a3802f"
   },
   "source": [
    "df_bandgaps.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Od2qlc1AuZd-",
    "outputId": "49e0b53c-ff78-4e96-d38e-7d6eb7881cb7"
   },
   "source": [
    "df_bandgaps.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49wvgH3jue6O",
    "outputId": "f1354bb1-2485-49a6-b815-01b8da9cea19"
   },
   "source": [
    "df_bandgaps.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 761
    },
    "collapsed": true,
    "id": "rMu5Y9N9ekw8",
    "outputId": "1422b23d-a8cc-4152-fdb1-155a4f6f0df0"
   },
   "source": [
    "# Visualizing the distribution of the band gaps.\n",
    "df_bandgaps['BG_PBE'].hist(bins=20)\n",
    "plt.xlabel('Bandgap Values (eV)')\n",
    "plt.ylabel('Values Count')\n",
    "plt.title('Distribution of Bandgaps')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "collapsed": true,
    "id": "r_No-A-Ohhiu",
    "outputId": "23bea683-8128-4f13-d975-8421b7245cca"
   },
   "source": [
    "# Box plot of BG_PBE\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(df_bandgaps['BG_PBE'])\n",
    "plt.title('Box Plot for BG_PBE')\n",
    "plt.ylabel('Feature Values')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "dXa3FtBqfTEm",
    "outputId": "f4270ed7-b2f8-4d3e-c787-ed1cfd7a45f0"
   },
   "source": [
    "# Correlation coefficients of BG_PBE against 47 numerical features.\n",
    "\n",
    "df_45 = df_45.set_index(['MOF'])\n",
    "df_bandgaps = df_bandgaps.set_index('MOF')\n",
    "df_45_bandgap_joined = df_45.join(df_bandgaps, how='outer', rsuffix='_bandgaps') #joining the 45 fingerprints with the bandgaps data\n",
    "\n",
    "numeric_cols = df_45_bandgap_joined.select_dtypes(include='number').columns\n",
    "\n",
    "df_spearman = df_45_bandgap_joined[[col for col in numeric_cols if col.endswith((\"_max\", \"_min\")) or col == 'BG_PBE']]\n",
    "df_pearson = df_45_bandgap_joined[[col for col in numeric_cols if not col.endswith((\"_max\", \"_min\"))]]\n",
    "\n",
    "spearman_matrix = df_spearman.corr(method='spearman')\n",
    "pearson_matrix = df_pearson.corr(method='pearson')\n",
    "\n",
    "bandgap_spearman_correlation = spearman_matrix['BG_PBE'].abs().sort_values(ascending=False)\n",
    "bandgap_pearson_correlation = pearson_matrix['BG_PBE'].abs().sort_values(ascending=False)\n",
    "print(\"Spearman:\")\n",
    "print(bandgap_spearman_correlation)\n",
    "print(\"Pearson:\")\n",
    "print(bandgap_pearson_correlation)\n",
    "\n",
    "# Plotting the correlation matrices\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(spearman_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Spearman Correlation Matrix for the 45 fingerprints')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(pearson_matrix, annot=True, cmap='coolwarm', fmt='.2f',annot_kws={\"size\": 8})\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.title('Pearson Correlation Matrix for the 45 fingerprints')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "GTfr7KFr4x4a",
    "outputId": "fd6f2229-1447-49fc-ecf5-08a4243862db"
   },
   "source": [
    "# These are the missing values in the 45 feature representation.\n",
    "\n",
    "df_45_bandgap_nan = df_45_bandgap_joined[df_45_bandgap_joined.isna().any(axis=1)]\n",
    "\n",
    "nan_columns = df_45_bandgap_nan.columns[df_45_bandgap_nan.isna().any()]\n",
    "df_45_bandgap_nan[nan_columns]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "id": "5YQ7k41zdirP",
    "outputId": "be49f471-c833-4f2e-b57a-bea05cb5521c"
   },
   "source": [
    "df_45.tail()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "collapsed": true,
    "id": "g1O02vjdzXlQ",
    "outputId": "63d1c159-38a5-4601-c544-86e486c730a1"
   },
   "source": [
    "# Scatter plots with correlation coefficients of BG_PBE against 47 features.\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "df_corr_plot = df_45_bandgap_joined.fillna(0) # there are missing values in some of the columns, here we zero them.\n",
    "\n",
    "numeric_cols = df_corr_plot.select_dtypes(include='number').columns\n",
    "\n",
    "num_cols = 6\n",
    "num_rows = (len(numeric_cols) + num_cols - 1) // num_cols\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(5*num_cols, 4*num_rows))\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    x = df_corr_plot['BG_PBE']\n",
    "    y = df_corr_plot[col]\n",
    "    if col.endswith((\"_max\", \"_min\")):\n",
    "        corr = ('Spearman', round(spearmanr(x, y)[0], 2))\n",
    "    else:\n",
    "        corr = ('Pearson', round(pearsonr(x, y)[0], 2))\n",
    "    ax = axes[i // num_cols][i % num_cols]\n",
    "    ax.scatter(x, y, label=f\"{corr[0]}={corr[1]}\", alpha=0.2)\n",
    "    ax.set_xlabel('BG_PBE')\n",
    "    ax.set_ylabel(col)\n",
    "    ax.legend()\n",
    "\n",
    "for j in range(len(numeric_cols), num_rows*num_cols):\n",
    "    fig.delaxes(axes[j // num_cols][j % num_cols])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3yRd1-ln7EE"
   },
   "source": [
    "**Exploring the 120 feature representation**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "GAeMy-URnxN0",
    "outputId": "ea60006a-63ba-4162-f530-b6579fb7302f"
   },
   "source": [
    "#df_120.head(3)\n",
    "#display all the columns, all of the 120 features\n",
    "pd.options.display.max_columns = None\n",
    "df_120.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NmATzKVyodjf",
    "outputId": "6ccbaa40-e2ee-4b53-fed0-f5412cccbcd8"
   },
   "source": [
    "df_120.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Q85VOZ1Osvut",
    "outputId": "5d17661b-795b-4588-dc95-1ab33defaebf"
   },
   "source": [
    "df_120.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "collapsed": true,
    "id": "3LZELQlyoBdP"
   },
   "source": [
    "#Plotting histograms of each of the 120 fingerprints.\n",
    "\n",
    "numeric_cols = df_120.select_dtypes(include='number').columns\n",
    "\n",
    "num_cols = 5\n",
    "num_rows = (len(numeric_cols) + num_cols - 1) // num_cols\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (col, ax) in enumerate(zip(numeric_cols, axes)):\n",
    "    df_45[col].hist(ax=ax, bins=20)\n",
    "    ax.set_title(col)\n",
    "    ax.set_xlabel('Values')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "for j in range(len(numeric_cols), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UImRaOw-p85s",
    "outputId": "12e7e4ef-f543-4f08-e8bd-e7f42df0970c"
   },
   "source": [
    "# Correlation coefficients of BG_PBE against 120 numerical features.\n",
    "\n",
    "df_120 = df_120.reset_index().set_index(['MOF'])\n",
    "df_bandgaps = df_bandgaps.reset_index().set_index('MOF')\n",
    "df_120_bandgap_joined = df_120.join(df_bandgaps, how='outer', rsuffix='_bandgaps') #joining the 120 fingerprints with the bandgaps data\n",
    "\n",
    "numeric_cols = df_120_bandgap_joined.select_dtypes(include='number').columns\n",
    "\n",
    "df_spearman = df_120_bandgap_joined[[col for col in numeric_cols if col.endswith((\"_max\", \"_min\")) or col == 'BG_PBE']]\n",
    "df_pearson = df_120_bandgap_joined[[col for col in numeric_cols if not col.endswith((\"_max\", \"_min\"))]]\n",
    "\n",
    "spearman_matrix = df_spearman.corr(method='spearman')\n",
    "pearson_matrix = df_pearson.corr(method='pearson')\n",
    "\n",
    "bandgap_spearman_correlation = spearman_matrix['BG_PBE'].abs().sort_values(ascending=False)\n",
    "bandgap_pearson_correlation = pearson_matrix['BG_PBE'].abs().sort_values(ascending=False)\n",
    "print(\"Spearman:\")\n",
    "print(bandgap_spearman_correlation)\n",
    "print(\"Pearson:\")\n",
    "print(bandgap_pearson_correlation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZgajaDWi7xE9"
   },
   "source": [
    "df_120.index"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NVQ2kwKn74q-"
   },
   "source": [
    "df_120.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aitm6gV4ni6k",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "outputId": "b160be0f-21cf-43f6-ed5d-c4c67c9f918b"
   },
   "source": [
    "# These are the missing values in the 120 feature representation.\n",
    "\n",
    "df_120_bandgap_nan = df_120_bandgap_joined[df_120_bandgap_joined.isna().any(axis=1)]\n",
    "\n",
    "nan_columns = df_120_bandgap_nan.columns[df_120_bandgap_nan.isna().any()]\n",
    "df_120_bandgap_nan[nan_columns]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QNZBv3C4rZZc"
   },
   "source": [
    "# Scatter plots with correlation coefficients of BG_PBE against 120 features.\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "df_corr_plot = df_120_bandgap_joined.fillna(0) # there are missing values in some of the columns, here we zero them.\n",
    "\n",
    "numeric_cols = df_corr_plot.select_dtypes(include='number').columns\n",
    "\n",
    "num_cols = 6\n",
    "num_rows = (len(numeric_cols) + num_cols - 1) // num_cols\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(5*num_cols, 4*num_rows))\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    x = df_corr_plot['BG_PBE']\n",
    "    y = df_corr_plot[col]\n",
    "    if col.endswith((\"_max\", \"_min\")):\n",
    "        corr = ('Spearman', round(spearmanr(x, y)[0], 2))\n",
    "    else:\n",
    "        corr = ('Pearson', round(pearsonr(x, y)[0], 2))\n",
    "    ax = axes[i // num_cols][i % num_cols]\n",
    "    ax.scatter(x, y, label=f\"{corr[0]}={corr[1]}\", alpha=0.2)\n",
    "    ax.set_xlabel('BG_PBE')\n",
    "    ax.set_ylabel(col)\n",
    "    ax.legend()\n",
    "\n",
    "for j in range(len(numeric_cols), num_rows*num_cols):\n",
    "    fig.delaxes(axes[j // num_cols][j % num_cols])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqQ8MR30rm05"
   },
   "source": [
    "**Exploring the sine representation**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dkTLpYtknxa5"
   },
   "source": [
    "#df_sine.head(3)\n",
    "#display all the columns, all of the 500 features\n",
    "pd.options.display.max_columns = None\n",
    "df_sine.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "33AVgwzgsCWI"
   },
   "source": [
    "df_sine.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h2qqDLMLsLT_"
   },
   "source": [
    "df_sine.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iX40EYBWtlbe"
   },
   "source": [
    "# Correlation coefficients of BG_PBE against sine numerical features.\n",
    "\n",
    "df_sine = df_sine.set_index(['MOF'])\n",
    "df_bandgaps = df_bandgaps.set_index('MOF')\n",
    "df_sine_bandgap_joined = df_sine.join(df_bandgaps, how='outer', rsuffix='_bandgaps') #joining the sine fingerprints with the bandgaps data\n",
    "\n",
    "numeric_cols = df_sine_bandgap_joined.select_dtypes(include='number').columns\n",
    "\n",
    "df_spearman = df_sine_bandgap_joined[[col for col in numeric_cols if col.endswith((\"_max\", \"_min\")) or col == 'BG_PBE']]\n",
    "df_pearson = df_sine_bandgap_joined[[col for col in numeric_cols if not col.endswith((\"_max\", \"_min\"))]]\n",
    "\n",
    "spearman_matrix = df_spearman.corr(method='spearman')\n",
    "pearson_matrix = df_pearson.corr(method='pearson')\n",
    "\n",
    "bandgap_spearman_correlation = spearman_matrix['BG_PBE'].abs().sort_values(ascending=False)\n",
    "bandgap_pearson_correlation = pearson_matrix['BG_PBE'].abs().sort_values(ascending=False)\n",
    "print(\"Spearman:\")\n",
    "print(bandgap_spearman_correlation)\n",
    "print(\"Pearson:\")\n",
    "print(bandgap_pearson_correlation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5cFoEGcctWyL"
   },
   "source": [
    "#These are the missing values in the sine feature representation.\n",
    "\n",
    "df_sine_bandgap_nan = df_sine_bandgap_joined[df_sine_bandgap_joined.isna().any(axis=1)]\n",
    "\n",
    "nan_columns = df_sine_bandgap_nan.columns[df_sine_bandgap_nan.isna().any()]\n",
    "df_sine_bandgap_nan[nan_columns]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9eEVSOvr1pO"
   },
   "source": [
    "**Exploring the ofm_fp representation**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Eihqj5-mn9qL"
   },
   "source": [
    "df_ofm_fp.head(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KwX4tWgms3k-"
   },
   "source": [
    "df_ofm_fp.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PHeUr_lTK1Gf"
   },
   "source": [
    "df_ofm_fp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d-W3nsxmvVXW"
   },
   "source": [
    "# Correlation coefficients of BG_PBE against ofm_fp numerical features.\n",
    "\n",
    "df_ofm_fp = df_ofm_fp.reset_index().set_index(['MOF'])\n",
    "df_bandgaps = df_bandgaps.reset_index().set_index('MOF')\n",
    "df_ofm_fp_bandgap_joined = df_ofm_fp.join(df_bandgaps, how='outer', rsuffix='_bandgaps') #joining the ofm_fp fingerprints with the bandgaps data\n",
    "\n",
    "numeric_cols = df_ofm_fp_bandgap_joined.select_dtypes(include='number').columns\n",
    "\n",
    "df_spearman = df_ofm_fp_bandgap_joined[[col for col in numeric_cols if col.endswith((\"_max\", \"_min\")) or col == 'BG_PBE']]\n",
    "df_pearson = df_ofm_fp_bandgap_joined[[col for col in numeric_cols if not col.endswith((\"_max\", \"_min\"))]]\n",
    "\n",
    "spearman_matrix = df_spearman.corr(method='spearman')\n",
    "pearson_matrix = df_pearson.corr(method='pearson')\n",
    "\n",
    "bandgap_spearman_correlation = spearman_matrix['BG_PBE'].abs().sort_values(ascending=False)\n",
    "bandgap_pearson_correlation = pearson_matrix['BG_PBE'].abs().sort_values(ascending=False)\n",
    "print(\"Spearman:\")\n",
    "print(bandgap_spearman_correlation)\n",
    "print(\"Pearson:\")\n",
    "print(bandgap_pearson_correlation)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pw8AxSKprOWv"
   },
   "source": [
    "#These are the missing values in the ofm_fp feature representation.\n",
    "\n",
    "df_ofm_fp_bandgap_nan = df_ofm_fp_bandgap_joined[df_ofm_fp_bandgap_joined.isna().any(axis=1)]\n",
    "\n",
    "nan_columns = df_ofm_fp_bandgap_nan.columns[df_ofm_fp_bandgap_nan.isna().any()]\n",
    "df_ofm_fp_bandgap_nan[nan_columns]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dVDSP1vyn9Cr"
   },
   "source": [
    "#df_qmof_refcodes.head()\n",
    "df_qmof_refcodes.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eex5Tfg7Ta3_"
   },
   "source": [
    "#plotting first 10 features of 3 random MOFS against each other"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YnQ8YvOHTZuW",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 720
    },
    "outputId": "eeb79c09-0807-40b4-a606-48f6457b398e"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('stoich45_fingerprints.csv')\n",
    "\n",
    "# Exclude the first row and first column\n",
    "data = data.iloc[1:, 1:]\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Randomly select three rows\n",
    "random_rows = data.sample(n=3, random_state=1)  # Set random_state for reproducibility\n",
    "\n",
    "# Extract the first 10 features (columns) from the selected rows\n",
    "selected_features = random_rows.iloc[:, :10].T\n",
    "\n",
    "# Plot the features\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each selected row with a different color\n",
    "colors = ['r', 'g', 'b']\n",
    "for idx, row in enumerate(selected_features.columns):\n",
    "    plt.plot(selected_features.index, selected_features[row], color=colors[idx], label=f'Row {row}')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Values')\n",
    "plt.title('First 10 Features for Three Random Rows')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRbKSwCWwsRX"
   },
   "source": [
    "# **Exploring 3 different dimensionality reduction techniques for the 45 and 120 fingerprints.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAj2bpCaxBAT"
   },
   "source": [
    "**1. PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ew-7uQYU525"
   },
   "source": [
    "45 PCA"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UtqgYu-6TPXK"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the dataset\n",
    "data = df_45_bandgap_joined\n",
    "\n",
    "# Exclude the first row and first column\n",
    "data = data.iloc[1:, 1:]\n",
    "\n",
    "# Ensure the data is numeric (if needed)\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Number of rows and columns before dropping NaN values\n",
    "initial_rows = data.shape[0]\n",
    "initial_columns = data.shape[1]\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data_clean = data.dropna()\n",
    "\n",
    "# Number of rows and columns after dropping NaN values\n",
    "final_rows = data_clean.shape[0]\n",
    "final_columns = data_clean.shape[1]\n",
    "\n",
    "# Calculate and print the number of dropped rows and columns\n",
    "dropped_rows = initial_rows - final_rows\n",
    "dropped_columns = initial_columns - final_columns\n",
    "print(f'Number of rows with NaN values dropped: {dropped_rows}')\n",
    "print(f'Number of columns with NaN values dropped: {dropped_columns}')\n",
    "\n",
    "# Splitting features and target variable\n",
    "X = data_clean.drop(['BG_PBE', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE'], axis=1)\n",
    "y = data_clean['BG_PBE']\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_norm, y, random_state=13, test_size=0.25, shuffle=True\n",
    ")\n",
    "\n",
    "# Applying PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(f'Explained variance ratio: {pca.explained_variance_ratio_}')\n",
    "\n",
    "# Plotting the PCA results\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', alpha=0.7)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Training Data')\n",
    "plt.colorbar(label='BG_PBE')\n",
    "plt.show()\n",
    "\n",
    "pca_full = PCA().fit(scaled_data)\n",
    "plt.plot(np.cumsum(pca_full.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfP8IVjdVznP"
   },
   "source": [
    "pca 120"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Oem4fa2LVDs4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "outputId": "108880bd-b187-4337-a217-2ff7ed3908b2"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the dataset\n",
    "data = df_120_bandgap_joined\n",
    "\n",
    "# Exclude the first row and first column\n",
    "data = data.iloc[1:, 1:]\n",
    "\n",
    "# Ensure the data is numeric (if needed)\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Number of rows and columns before dropping NaN values\n",
    "initial_rows = data.shape[0]\n",
    "initial_columns = data.shape[1]\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data_clean = data.dropna()\n",
    "\n",
    "# Number of rows and columns after dropping NaN values\n",
    "final_rows = data_clean.shape[0]\n",
    "final_columns = data_clean.shape[1]\n",
    "\n",
    "# Calculate and print the number of dropped rows and columns\n",
    "dropped_rows = initial_rows - final_rows\n",
    "dropped_columns = initial_columns - final_columns\n",
    "print(f'Number of rows with NaN values dropped: {dropped_rows}')\n",
    "print(f'Number of columns with NaN values dropped: {dropped_columns}')\n",
    "\n",
    "# Splitting features and target variable\n",
    "X = data_clean.drop(['BG_PBE', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE'], axis=1)\n",
    "y = data_clean['BG_PBE']\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_norm, y, random_state=13, test_size=0.25, shuffle=True\n",
    ")\n",
    "\n",
    "# Applying PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(f'Explained variance ratio: {pca.explained_variance_ratio_}')\n",
    "\n",
    "# Plotting the PCA results\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', alpha=0.7)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Training Data')\n",
    "plt.colorbar(label='BG_PBE')\n",
    "plt.show()\n",
    "\n",
    "pca_full = PCA().fit(scaled_data)\n",
    "plt.plot(np.cumsum(pca_full.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Cr0UYNo8WNF4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "outputId": "d7f819d2-cd8f-49b4-9f94-23648d4dd58d"
   },
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the dataset\n",
    "df_45_120_bandgap_joined_clean = df_120_bandgap_joined_clean.join(df_45_bandgap_joined_clean.drop(columns=['BG_PBE', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE']), how='right', rsuffix='_120')\n",
    "data = df_45_120_bandgap_joined_clean.copy()\n",
    "\n",
    "# Exclude the first row and first column\n",
    "data = data.iloc[1:, 1:]\n",
    "\n",
    "# Ensure the data is numeric (if needed)\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Number of rows and columns before dropping NaN values\n",
    "initial_rows = data.shape[0]\n",
    "initial_columns = data.shape[1]\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data_clean = data.dropna()\n",
    "\n",
    "# Number of rows and columns after dropping NaN values\n",
    "final_rows = data_clean.shape[0]\n",
    "final_columns = data_clean.shape[1]\n",
    "\n",
    "# Calculate and print the number of dropped rows and columns\n",
    "dropped_rows = initial_rows - final_rows\n",
    "dropped_columns = initial_columns - final_columns\n",
    "print(f'Number of rows with NaN values dropped: {dropped_rows}')\n",
    "print(f'Number of columns with NaN values dropped: {dropped_columns}')\n",
    "\n",
    "# Splitting features and target variable\n",
    "X = data_clean.drop(['BG_PBE', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE'], axis=1)\n",
    "y = data_clean['BG_PBE']\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_norm, y, random_state=13, test_size=0.25, shuffle=True\n",
    ")\n",
    "\n",
    "# Applying PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(f'Explained variance ratio: {pca.explained_variance_ratio_}')\n",
    "\n",
    "# Plotting the PCA results\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', alpha=0.7)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Training Data')\n",
    "plt.colorbar(label='BG_PBE')\n",
    "plt.show()\n",
    "\n",
    "pca_full = PCA().fit(scaled_data)\n",
    "plt.plot(np.cumsum(pca_full.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2A5r2bQxPQF"
   },
   "source": [
    "**2. t-SNE**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CvQTKC_-xfJM"
   },
   "source": [
    "# Cleaning the 45.\n",
    "df_45_bandgap_nan = df_45_bandgap_joined[df_45_bandgap_joined.isna().any(axis=1)]\n",
    "\n",
    "nan_columns = df_45_bandgap_nan.columns[df_45_bandgap_nan.isna().any()]\n",
    "df_45_bandgap_joined_clean = df_45_bandgap_joined[~df_45_bandgap_joined.isna().any(axis=1)]\n",
    "\n",
    "assert df_45_bandgap_joined_clean.shape[0] == df_45_bandgap_joined.shape[0] - df_45_bandgap_joined[df_45_bandgap_joined.isna().any(axis=1)].shape[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DsJ6ucI1yCE4"
   },
   "source": [
    "# t-SNE 45\n",
    "df = df_45_bandgap_joined_clean.copy()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop(['BG_PBE', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE'], axis=1)\n",
    "y = df['BG_PBE']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "\n",
    "X_train_45, X_test_45, y_train_45, y_test_45 = train_test_split(\n",
    "    X_norm, y, random_state=42, test_size=0.2, shuffle=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GEnkdhFhyKBh"
   },
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=50)\n",
    "X_train_45_tsne = tsne.fit_transform(X_train_45)\n",
    "\n",
    "tsne.kl_divergence_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YnapcR4lyRoq"
   },
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(x=X_train_45_tsne[:, 0], y=X_train_45_tsne[:, 1], color=y_train_45)\n",
    "fig.update_layout(\n",
    "    title=\"t-SNE visualization of 45_bandgap_joined with perplexity 50\",\n",
    "    xaxis_title=\"First t-SNE\",\n",
    "    yaxis_title=\"Second t-SNE\",\n",
    ")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "Nlxvtms4x4aE"
   },
   "source": [
    "# Cleaning the 120\n",
    "df_120_bandgap_joined_clean = df_120_bandgap_joined[~df_120_bandgap_joined.isna().any(axis=1)]\n",
    "\n",
    "assert df_120_bandgap_joined_clean.shape[0] == df_120_bandgap_joined.shape[0] - df_120_bandgap_joined[df_120_bandgap_joined.isna().any(axis=1)].shape[0]\n",
    "\n",
    "df_120_bandgap_joined[df_120_bandgap_joined.isna().any(axis=1)].shape[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qB0tsXTdyWUP"
   },
   "source": [
    "# t-SNE for 120\n",
    "df = df_120_bandgap_joined_clean.copy()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop(['BG_PBE', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE'], axis=1)\n",
    "y = df['BG_PBE']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "\n",
    "X_train_120, X_test_120, y_train_120, y_test_120 = train_test_split(\n",
    "    X_norm, y, random_state=13, test_size=0.25, shuffle=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6wmpPUY9ycjw"
   },
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=50)\n",
    "X_train_120_tsne = tsne.fit_transform(X_train_120)\n",
    "\n",
    "tsne.kl_divergence_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_CSpRweby8qg"
   },
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(x=X_train_120_tsne[:, 0], y=X_train_120_tsne[:, 1], color=y_train_120)\n",
    "fig.update_layout(\n",
    "    title=\"t-SNE visualization of 120_bandgap_joined with perplexity 50\",\n",
    "    xaxis_title=\"First t-SNE\",\n",
    "    yaxis_title=\"Second t-SNE\",\n",
    ")\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DaKZyiQxW1VC"
   },
   "source": [
    "df_120_bandgap_joined_clean.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mc7vWRteW9Gn"
   },
   "source": [
    "df_45_bandgap_joined_clean.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dSMIWX-uXUz2"
   },
   "source": [
    "df_test1 = pd.DataFrame({\n",
    "    'MOF': ['m1', 'm2', 'm3', 'm4'],\n",
    "    'column1': [1,2,3,7]\n",
    "})\n",
    "\n",
    "df_test2 = pd.DataFrame({\n",
    "    'MOF': ['m1', 'm2', 'm3', 'm4', 'm5'],\n",
    "    'column2': [8,6,9,1,10]\n",
    "})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Nt9ii5qnYge3"
   },
   "source": [
    "df_test1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CC2JQ9ivYksl"
   },
   "source": [
    "df_test2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2CDipYfJX8XZ"
   },
   "source": [
    "df_test2.set_index('MOF').join(df_test1.set_index('MOF'), how='left')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u4bWi6v69IBh"
   },
   "source": [
    "# t-SNE for 45 and 120 combined\n",
    "df_45_120_bandgap_joined_clean = df_120_bandgap_joined_clean.join(df_45_bandgap_joined_clean.drop(columns=['BG_PBE', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE']), how='right', rsuffix='_120')\n",
    "df = df_45_120_bandgap_joined_clean.copy()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop(['BG_PBE', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE'], axis=1)\n",
    "y = df['BG_PBE']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "\n",
    "X_train_45_120, X_test_45_120, y_train_45_120, y_test_45_120 = train_test_split(\n",
    "    X_norm, y, random_state=13, test_size=0.25, shuffle=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iX9dlBUsOrea"
   },
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=50)\n",
    "X_train_45_120_tsne = tsne.fit_transform(X_train_45_120)\n",
    "\n",
    "tsne.kl_divergence_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qcjckCOFO34p"
   },
   "source": [
    "import plotly.express as px\n",
    "# import kaleido\n",
    "\n",
    "fig = px.scatter(x=X_train_45_120_tsne[:, 0], y=X_train_45_120_tsne[:, 1], color=y_train_45_120)\n",
    "fig.update_layout(\n",
    "    title=\"t-SNE visualization of 45_120_bandgap_joined with perplexity 50\",\n",
    "    xaxis_title=\"First t-SNE\",\n",
    "    yaxis_title=\"Second t-SNE\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsaJ2aBUxaqG"
   },
   "source": [
    "**3. UMAP**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SRrLkMCCw8Q3"
   },
   "source": [
    "# Ensure the 'MOF' columns in df_45 and df_120 are set as the index for joining\n",
    "df_45.set_index('MOF', inplace=True)\n",
    "df_120.set_index('MOF', inplace=True)\n",
    "df_bandgaps.set_index('refcode', inplace=True)\n",
    "\n",
    "# Join the datasets with bandgaps information based on the index\n",
    "joined_df_45 = df_45.join(bandgaps_df[['BG_PBE']])\n",
    "joined_df_120 = df_120.join(bandgaps_df[['BG_PBE']])\n",
    "\n",
    "# Reset the index to turn 'MOF' back into a column\n",
    "joined_df_45.reset_index(inplace=True)\n",
    "joined_df_120.reset_index(inplace=True)\n",
    "\n",
    "# Check for NaN values and remove them\n",
    "joined_df_45 = joined_df_45.dropna()\n",
    "joined_df_120 = joined_df_120.dropna()\n",
    "\n",
    "# Ensure 'BG_PBE' is in the columns\n",
    "assert 'BG_PBE' in joined_df_45.columns, \"BG_PBE not found in joined_df_45\"\n",
    "assert 'BG_PBE' in joined_df_120.columns, \"BG_PBE not found in joined_df_120\"\n",
    "\n",
    "# Select only numeric columns for UMAP, excluding 'BG_PBE'\n",
    "features_45 = joined_df_45.select_dtypes(include=[float, int]).drop(columns=['BG_PBE'])\n",
    "features_120 = joined_df_120.select_dtypes(include=[float, int]).drop(columns=['BG_PBE'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kIZvN1TgCDYR"
   },
   "source": [
    "# Select only numeric columns for UMAP, excluding 'BG_PBE'\n",
    "umap_45 = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean').fit_transform(features_45)\n",
    "umap_120 = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean').fit_transform(features_120)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3l0lB_vMCGqt"
   },
   "source": [
    "# Visualization function\n",
    "def plot_umap(embedding, labels, title, label_name):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=labels, cmap='Spectral', s=5)\n",
    "    plt.colorbar(scatter, label=label_name)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('UMAP 1')\n",
    "    plt.ylabel('UMAP 2')\n",
    "    plt.show()\n",
    "\n",
    "# Plot UMAP results with color mapping\n",
    "plot_umap(umap_45, joined_df_45['BG_PBE'], 'Stoichiometric-45', 'Bandgap (eV)')\n",
    "plot_umap(umap_120, joined_df_120['BG_PBE'], 'Stoichiometric-120', 'Bandgap (eV)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ndECx9KvCJPj"
   },
   "source": [
    "# Set the 'MOF' and 'refcode' columns as the index for joining\n",
    "df_45.set_index('MOF', inplace=True)\n",
    "df_120.set_index('MOF', inplace=True)\n",
    "bandgaps_df.set_index('refcode', inplace=True)\n",
    "\n",
    "# Join the datasets with bandgaps information using an inner join\n",
    "joined_df_45 = df_45.join(bandgaps_df[['BG_PBE']], how='inner')\n",
    "joined_df_120 = df_120.join(bandgaps_df[['BG_PBE']], how='inner')\n",
    "\n",
    "# Reset the index to turn 'MOF' back into a column\n",
    "joined_df_45.reset_index(inplace=True)\n",
    "joined_df_120.reset_index(inplace=True)\n",
    "\n",
    "# Rename columns to prevent conflicts\n",
    "joined_df_45 = joined_df_45.rename(columns=lambda x: x + '_45' if x != 'MOF' else x)\n",
    "joined_df_120 = joined_df_120.rename(columns=lambda x: x + '_120' if x != 'MOF' else x)\n",
    "\n",
    "# Merge the two dataframes on 'MOF'\n",
    "combined_df = pd.merge(joined_df_45, joined_df_120, on='MOF', how='inner')\n",
    "\n",
    "# Check for NaN values and remove them\n",
    "print(\"\\nNaN values in combined_df before dropping:\\n\", combined_df.isna().sum())\n",
    "combined_df = combined_df.dropna()\n",
    "print(\"\\nSize of combined_df after dropping NaN:\", combined_df.shape)\n",
    "print(\"NaN values in combined_df after dropping:\\n\", combined_df.isna().sum())\n",
    "\n",
    "# Select only numeric columns for UMAP, excluding 'BG_PBE_45' and 'BG_PBE_120'\n",
    "features_combined = combined_df.select_dtypes(include=[float, int]).drop(columns=['BG_PBE_45', 'BG_PBE_120'])\n",
    "\n",
    "# Check the size of features_combined\n",
    "print(\"\\nSize of features_combined:\", features_combined.shape)\n",
    "print(\"NaN values in features_combined:\\n\", features_combined.isna().sum())\n",
    "\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Perform UMAP on the combined dataset\n",
    "if features_combined.shape[0] > 0:\n",
    "    umap_combined = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean').fit_transform(features_combined)\n",
    "\n",
    "    # Visualization function\n",
    "    def plot_umap(embedding, labels, title, label_name):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=labels, cmap='Spectral', s=5)\n",
    "        plt.colorbar(scatter, label=label_name)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('UMAP 1')\n",
    "        plt.ylabel('UMAP 2')\n",
    "        plt.show()\n",
    "\n",
    "    # Plot combined UMAP results\n",
    "    plot_umap(umap_combined, combined_df['BG_PBE_45'], 'Combined Stoichiometric-45 and 120', 'Bandgap (eV)')\n",
    "else:\n",
    "    print(\"No data available for UMAP after dropping NaN values.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTZywjXGQDrl"
   },
   "source": [
    "**Visuallization of comparison of 3 random MOFs against each other with 10 feautures.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KKO6NSB8d7VM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "outputId": "4c765828-7243-4230-e786-402590971004"
   },
   "source": [
    "df_45_bandgap_joined_clean.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vcCNdxRqebrL"
   },
   "source": [
    "df_45_bandgap_joined_clean.index[5000:6000]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PHg3nXcgQQfV"
   },
   "source": [
    "df = df_45_bandgap_joined_clean.reset_index()\n",
    "# Select the MOFs and features\n",
    "selected_mofs = [\n",
    "    'ABALOF_FSR',\n",
    "    'IPEBIO01_FSR',\n",
    "    'tobacco_srsb_sym_3_on_1_sym_3_mc_0_L_6'\n",
    "]\n",
    "selected_features = [\n",
    "    'electronegativity_mean',\n",
    "    'electron_affinity_mean',\n",
    "    'boiling_mean',\n",
    "    'ionization_energy_mean',\n",
    "    'melting_mean',\n",
    "    # 'Feature6',\n",
    "    # 'Feature7',\n",
    "    # 'Feature8',\n",
    "    # 'Feature9',\n",
    "    # 'Feature10'\n",
    "  ]\n",
    "\n",
    "# Filter the dataframe\n",
    "filtered_df = df[df['MOF'].isin(selected_mofs)][selected_features + ['MOF']]\n",
    "\n",
    "# Set the MOF_ID as the index for easier plotting\n",
    "filtered_df.set_index('MOF', inplace=True)\n",
    "\n",
    "# Transpose the dataframe to get features on the x-axis\n",
    "filtered_df = filtered_df.T\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for mof in selected_mofs:\n",
    "    plt.plot(filtered_df.index, filtered_df[mof], marker='o', label=mof)\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Comparison of Selected Features for 3 MOFs')\n",
    "plt.legend(title='MOFs')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ7VeJrezAy4"
   },
   "source": [
    "# **Training models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSei2gy-PrfD"
   },
   "source": [
    " **1. Training on the 45_bandgap_clean dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ldcku4xxgdcq"
   },
   "source": [
    "# Splitting the 45_bandgap_clean df into train, validation and test\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "df = df_45_bandgap_joined_clean.reset_index().drop(columns=['MOF', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE'])\n",
    "\n",
    "X = df.drop(columns=['BG_PBE'])\n",
    "y = df['BG_PBE']\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#neural network"
   ],
   "metadata": {
    "id": "kq0mkh23l6uN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pip uninstall tensorflow\n",
    "pip install tensorflow"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "8Vm-rwLSO24X",
    "outputId": "87c9836b-67bb-4f38-b2df-45dfa05c920b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = df_45_bandgap_joined_clean.reset_index().drop(columns=['MOF', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE'])\n",
    "\n",
    "# Exclude the first row and first column\n",
    "data = data.iloc[1:, 1:]\n",
    "\n",
    "# Ensure the data is numeric\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data_clean = data.dropna()\n",
    "\n",
    "# Split features and target variable\n",
    "X = data_clean.drop(columns=['BG_PBE'])\n",
    "y = data_clean['BG_PBE']\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Applying PCA for dimensionality reduction (optional)\n",
    "#pca = PCA(n_components=20)  # Increase the number of components for more variance retention\n",
    "#X_train_pca = pca.fit_transform(X_train)\n",
    "#X_valid_pca = pca.transform(X_valid)\n",
    "#X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Define the neural network model with additional layers and dropout\n",
    "model = Sequential([\n",
    "    Dense(256, input_dim=X_train_pca.shape[1], activation='relu'),  # Increase number of neurons\n",
    "    Dropout(0.3),  # Increase dropout rate\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model with a reduced learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')  # Adjust learning rate\n",
    "\n",
    "# Callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)  # Increase patience\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, min_lr=1e-6)  # Adjust patience and min_lr\n",
    "\n",
    "# Train the model with increased epochs\n",
    "history = model.fit(X_train_pca, y_train, validation_data=(X_valid_pca, y_valid), epochs=500, batch_size=32, verbose=1,\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test_pca, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Calculate regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R Score:\", r2)\n",
    "\n",
    "# Plot the loss over epochs\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of predictions vs true values\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.xlabel('True BG_PBE')\n",
    "plt.ylabel('Predicted BG_PBE')\n",
    "plt.title('Predicted vs True BG_PBE')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Line of perfect prediction\n",
    "plt.show()\n",
    "\n",
    "# Applying PCA to the full scaled data for the cumulative explained variance plot\n",
    "pca_full = PCA().fit(X_train)\n",
    "plt.plot(np.cumsum(pca_full.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "QpkR_sSpNUW_",
    "outputId": "510e77df-aa2e-4c8e-85cc-758da6b958cf"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# without pca\n"
   ],
   "metadata": {
    "id": "4bZe7j8kU3t-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = df_45_bandgap_joined_clean.reset_index().drop(columns=['MOF', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE'])\n",
    "\n",
    "# Exclude the first row and first column\n",
    "data = data.iloc[1:, 1:]\n",
    "\n",
    "# Ensure the data is numeric\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data_clean = data.dropna()\n",
    "\n",
    "# Split features and target variable\n",
    "X = data_clean.drop(columns=['BG_PBE'])\n",
    "y = data_clean['BG_PBE']\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network model with additional layers and dropout\n",
    "model = Sequential([\n",
    "    Dense(256, input_dim=X_train.shape[1], activation='relu'),  # Increase number of neurons\n",
    "    Dropout(0.3),  # Increase dropout rate\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model with a reduced learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')  # Adjust learning rate\n",
    "\n",
    "# Callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)  # Increase patience\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, min_lr=1e-6)  # Adjust patience and min_lr\n",
    "\n",
    "# Train the model with increased epochs\n",
    "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=500, batch_size=32, verbose=1,\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R Score:\", r2)\n",
    "\n",
    "# Plot the loss over epochs\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of predictions vs true values\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.xlabel('True BG_PBE')\n",
    "plt.ylabel('Predicted BG_PBE')\n",
    "plt.title('Predicted vs True BG_PBE')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Line of perfect prediction\n",
    "plt.show()\n",
    "#"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "IRnmJadfUtQK",
    "outputId": "759af0ee-eb6e-4df2-ad29-df1a46a4db06"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# automated hypertuning"
   ],
   "metadata": {
    "id": "5JnRYd-BWB-A"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "67pkSPSbWPqF"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "pip install keras-tuner\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcKB0uJ7WPlL",
    "outputId": "d0803bb4-d975-4a0c-ec11-de24d2fc9dde"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kerastuner\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from kerastuner import RandomSearch\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = df_45_bandgap_joined_clean.reset_index().drop(columns=['MOF', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE'])\n",
    "\n",
    "# Exclude the first row and first column\n",
    "data = data.iloc[1:, 1:]\n",
    "\n",
    "# Ensure the data is numeric\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data_clean = data.dropna()\n",
    "\n",
    "# Split features and target variable\n",
    "X = data_clean.drop(columns=['BG_PBE'])\n",
    "y = data_clean['BG_PBE']\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network model with additional layers, batch normalization, and dropout\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hp.Int('units_1', min_value=64, max_value=512, step=32), input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout_1', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(hp.Int('units_2', min_value=64, max_value=512, step=32), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout_2', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(hp.Int('units_3', min_value=32, max_value=256, step=32), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout_3', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])),\n",
    "                  loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuning\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='helloworld')\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, min_lr=1e-6)\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=500, validation_data=(X_valid, y_valid), callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = best_model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R Score:\", r2)\n",
    "\n",
    "# Plot the loss over epochs\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(tuner.oracle.get_best_trials(num_trials=1)[0].metrics.get_history('loss'), label='Train Loss')\n",
    "plt.plot(tuner.oracle.get_best_trials(num_trials=1)[0].metrics.get_history('val_loss'), label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of predictions vs true values\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.xlabel('True BG_PBE')\n",
    "plt.ylabel('Predicted BG_PBE')\n",
    "plt.title('Predicted vs True BG_PBE')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Line of perfect prediction\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "z_aANTlfWEcA",
    "outputId": "edf2fea4-9022-4c1f-84ba-98b4c0167371"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#after plot  fixing"
   ],
   "metadata": {
    "id": "38PszfgdAS_f"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = df_45_bandgap_joined_clean.reset_index().drop(columns=['MOF', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE'])\n",
    "\n",
    "# Exclude the first row and first column\n",
    "data = data.iloc[1:, 1:]\n",
    "\n",
    "# Ensure the data is numeric\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data_clean = data.dropna()\n",
    "\n",
    "# Split features and target variable\n",
    "X = data_clean.drop(columns=['BG_PBE'])\n",
    "y = data_clean['BG_PBE']\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network model with additional layers, batch normalization, and dropout\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hp.Int('units_1', min_value=64, max_value=512, step=32), input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout_1', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(hp.Int('units_2', min_value=64, max_value=512, step=32), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout_2', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(hp.Int('units_3', min_value=32, max_value=256, step=32), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(hp.Float('dropout_3', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])),\n",
    "                  loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Hyperparameter tuning\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='helloworld')\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, min_lr=1e-6)\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=500, validation_data=(X_valid, y_valid), callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = best_model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R Score:\", r2)\n",
    "\n",
    "# Plot the loss over epochs\n",
    "best_trial = tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "history = best_trial.metrics.get_history()\n",
    "\n",
    "train_loss = [metric.get('value') for metric in history['loss']]\n",
    "val_loss = [metric.get('value') for metric in history['val_loss']]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of predictions vs true values\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.xlabel('True BG_PBE')\n",
    "plt.ylabel('Predicted BG_PBE')\n",
    "plt.title('Predicted vs True BG_PBE')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Line of perfect prediction\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "YvnFMMbsAWzZ",
    "outputId": "62e35b44-95bf-4f00-bb6a-f47bf7334aeb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "data = df_45_bandgap_joined_clean.reset_index().drop(columns=['MOF', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE'])\n",
    "\n",
    "# Exclude the first row and first column\n",
    "data = data.iloc[1:, 1:]\n",
    "\n",
    "# Ensure the data is numeric\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Number of rows and columns before dropping NaN values\n",
    "initial_rows = data.shape[0]\n",
    "initial_columns = data.shape[1]\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data_clean = data.dropna()\n",
    "\n",
    "# Number of rows and columns after dropping NaN values\n",
    "final_rows = data_clean.shape[0]\n",
    "final_columns = data_clean.shape[1]\n",
    "\n",
    "# Calculate and print the number of dropped rows and columns\n",
    "dropped_rows = initial_rows - final_rows\n",
    "dropped_columns = initial_columns - final_columns\n",
    "print(f'Number of rows with NaN values dropped: {dropped_rows}')\n",
    "print(f'Number of columns with NaN values dropped: {dropped_columns}')\n",
    "\n",
    "# Check if essential columns exist after cleaning\n",
    "#essential_columns = ['BG_PBE', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE']\n",
    "#missing_columns = [col for col in essential_columns if col not in data_clean.columns]\n",
    "#if missing_columns:\n",
    "#    raise ValueError(f\"The following essential columns are missing after cleaning: {missing_columns}\")\n",
    "\n",
    "# Splitting features and target variable\n",
    "#X = data_clean.drop(essential_columns, axis=1)\n",
    "#y = data_clean['BG_PBE']\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "#    X_norm, y, random_state=13, test_size=0.25, shuffle=True\n",
    "#)\n",
    "\n",
    "# Applying PCA for dimensionality reduction (optional)\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_pca.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer for regression\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_pca, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test_pca, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Plotting the PCA results\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', alpha=0.7)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Training Data')\n",
    "plt.colorbar(label='BG_PBE')\n",
    "plt.show()\n",
    "\n",
    "# Applying PCA to the full scaled data for the cumulative explained variance plot\n",
    "pca_full = PCA().fit(X_norm)\n",
    "plt.plot(np.cumsum(pca_full.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R Score:\", r2)\n",
    "\n",
    "# Plot the loss over epochs\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#mse = mean_squared_error(y_valid, y_pred)\n",
    "#r2 = r2_score(y_valid, y_pred)\n",
    "\n",
    "#print(f'Mean Squared Error: {mse}')\n",
    "#print(f'R^2 Score: {r2}')\n",
    "#from sklearn.metrics import precision_score\n",
    "#precision = precision_score(y_valid, y_pred)\n",
    "#print(f\"Precision: {precision:.2f}\")\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#accuracy = accuracy_score(y_valid, y_pred)\n",
    "#print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Scatter plot of predictions vs true values\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.xlabel('True BG_PBE')\n",
    "plt.ylabel('Predicted BG_PBE')\n",
    "plt.title('Predicted vs True BG_PBE')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)  # Line of perfect prediction\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SEleGUyel588",
    "outputId": "dc3a774b-4a92-420e-9bd6-d758e1d4f50f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = df_45_bandgap_joined_clean.reset_index().drop(columns=['MOF', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE'])\n",
    "\n",
    "# Exclude the first row and first column\n",
    "data = data.iloc[1:, 1:]\n",
    "\n",
    "# Ensure the data is numeric\n",
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data_clean = data.dropna()\n",
    "\n",
    "# Split features and target variable\n",
    "X = data_clean.drop(columns=['BG_PBE'])\n",
    "y = data_clean['BG_PBE']\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Applying PCA for dimensionality reduction (optional)\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_valid_pca = pca.transform(X_valid)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train_pca.shape[1], activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_pca, y_train, validation_data=(X_valid_pca, y_valid), epochs=200, batch_size=32, verbose=1,\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test_pca, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Calculate regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R Score:\", r2)\n",
    "\n",
    "# Plot the loss over epochs\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of predictions vs true values\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.xlabel('True BG_PBE')\n",
    "plt.ylabel('Predicted BG_PBE')\n",
    "plt.title('Predicted vs True BG_PBE')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Line of perfect prediction\n",
    "plt.show()\n",
    "\n",
    "# Applying PCA to the full scaled data for the cumulative explained variance plot\n",
    "pca_full = PCA().fit(X_norm)\n",
    "plt.plot(np.cumsum(pca_full.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs. Number of Components')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-qjwrcm_wIBL",
    "outputId": "1cfc07f1-edd9-409c-f361-2980a8379047"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convolutional neural network"
   ],
   "metadata": {
    "id": "fohYGGjKmIlp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = df_45_bandgap_joined_clean.reset_index().drop(columns=['MOF', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE'])\n",
    "\n",
    "# Exclude the first row and first column\n",
    "data = data.iloc[1:, 1:]\n",
    "\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data_clean = data.dropna()\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data for CNN input\n",
    "X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
    "X_valid = X_valid.reshape(-1, X_valid.shape[1], 1)\n",
    "X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
    "\n",
    "# Create a neural network model for regression\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    tf.keras.layers.MaxPooling1D(2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=100, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R Score:\", r2)\n",
    "\n",
    "# Plot the loss over epochs\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of predictions vs true values\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.xlabel('True BG_PBE')\n",
    "plt.ylabel('Predicted BG_PBE')\n",
    "plt.title('Predicted vs True BG_PBE')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Line of perfect prediction\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hvwCXjK2mLhd",
    "outputId": "18a41697-5995-4df6-9593-9cc7be5332af"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = df_45_bandgap_joined_clean.reset_index().drop(columns=['MOF', 'CBM_PBE', 'VBM_PBE', 'Direct_PBE'])\n",
    "\n",
    "# Exclude the first row and first column\n",
    "data = data.iloc[1:, 1:]\n",
    "\n",
    "# Drop rows with NaN values\n",
    "data_clean = data.dropna()\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data_clean.drop(columns=['BG_PBE'])\n",
    "y = data_clean['BG_PBE']\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "#X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "X = X.sample(frac=1, axis=1, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data for CNN input\n",
    "X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
    "X_valid = X_valid.reshape(-1, X_valid.shape[1], 1)\n",
    "X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
    "\n",
    "# Create a neural network model for regression\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    tf.keras.layers.MaxPooling1D(2),\n",
    "    tf.keras.layers.Conv1D(128, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=200, batch_size=64, callbacks=[lr_scheduler, early_stopping], verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R Score:\", r2)\n",
    "\n",
    "# Plot the loss over epochs\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of predictions vs true values\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.xlabel('True BG_PBE')\n",
    "plt.ylabel('Predicted BG_PBE')\n",
    "plt.title('Predicted vs True BG_PBE')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Line of perfect prediction\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mT_noBqEVfWR",
    "outputId": "da48bc1f-64e5-4ecb-ab09-e37716910b7a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvwen-LhWPfK"
   },
   "source": [
    "**Selection of Models** : Random Forest, Gaussian Process Regressor, XGB Regressor, Multilayer perceptron NN, Ridge Regression, Lasso."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qjHS5MLSpXJA"
   },
   "source": [
    "'''\n",
    "Shortlist the best performing models. We aim to choose 3 models to proceed further.\n",
    "'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"regressor\", None)\n",
    "                    ])\n",
    "\n",
    "param_grid = {'regressor': [\n",
    "                             RandomForestRegressor(n_estimators=100),\n",
    "                             GaussianProcessRegressor(),\n",
    "                             Ridge(),\n",
    "                             Lasso(),\n",
    "                             XGBRegressor(),\n",
    "                             MLPRegressor()\n",
    "                             ]\n",
    "              }\n",
    "\n",
    "grid_search = GridSearchCV(pipeline,\n",
    "                           param_grid,\n",
    "                           cv=5,\n",
    "                           scoring='r2',\n",
    "                           return_train_score=True\n",
    "                           )\n",
    "grid_search.fit(X_train_val, y_train_val)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gZE6bDBkrMcX"
   },
   "source": [
    "grid_search.best_estimator_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a9D0XdeyrLQi"
   },
   "source": [
    "grid_search.cv_results_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbVtTAQNgUqE"
   },
   "source": [
    " 'mean_test_score': array([ 6.94196553e-01, -3.36830726e+03,  5.12576164e-01, -1.84752611e-04,\n",
    "         6.84328776e-01,  6.55321024e-01]),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7OdtWFFhGFc"
   },
   "source": [
    "Performance of models on the 45_bandgap:\n",
    "\n",
    "1.   Random Forest: 0,69\n",
    "2.   XGB Regressor: 0,68\n",
    "3.   MLP Regressor: 0,65\n",
    "4.   Ridge: 0,51\n",
    "5.   Lasso: -0,0001\n",
    "6.   Gaussian Process Regressor: -3000\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdSIisDtSG7h"
   },
   "source": [
    "Training on Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1XXZumLGB_oH"
   },
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "pipe_pca = Pipeline([('PCA', pca),\n",
    "                 ('regressor', XGBRegressor())])\n",
    "\n",
    "cv_xgb_pca = cross_validate(pipe_pca, X_train_val, y_train_val, cv=5, n_jobs=-1)\n",
    "cv_xgb_pca['test_score']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FWaq9_eNUZ-7"
   },
   "source": [
    "#XGB Regressor with PCA 45 Hyperparameter optimisation\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "param_grid = {\n",
    "    #\"regressor__n_estimators\":[100, 300, 500] ,\n",
    "    #\"regressor__max_depth\": [6, 8, 10],\n",
    "    #\"regressor__min_samples_split\": 5,\n",
    "    #\"regressor__learning_rate\": [0.1, 0.05, 0.01],\n",
    "    #\"regressor__loss\": \"squared_error\",\n",
    "}\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "reg = Pipeline([('PCA', pca),\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"regressor\", XGBRegressor(n_estimators=500, max_depth=8, learning_rate=0.05)) #GradientBoostingRegressor())\n",
    "                    ])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=reg, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1, return_train_score=True)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Retrieve the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best parameters found: {best_params}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iXo-tvdCp4Q"
   },
   "source": [
    "**Perfomance of the XGB Regressor with PCA on the 45df with optimal hyperparameters on the validation set is: 0.62145388**\n",
    "\n",
    "**Optimal hyperparameters are: n_estimators=500, max_depth=8, learning_rate=0.05**\n",
    "\n",
    "Evalutaion metric for hyperparameter optimisation used is r^2."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sm1jJq0ABqLu"
   },
   "source": [
    "# Do this at the end for XGB\n",
    "reg = Pipeline([('PCA', pca),\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"regressor\", XGBRegressor(n_estimators=300, max_depth=8, learning_rate=0.05)) # insert your best parameters here\n",
    "                    ])\n",
    "\n",
    "reg.fit(X_train_val, y_train_val)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "SUWdVs0MCXLR"
   },
   "source": [
    "x = y_test\n",
    "y = y_pred\n",
    "\n",
    "plt.scatter(x, y, alpha=0.4, label=f\"r^2 = {round(r2, 2)}, RMSE = {round(np.sqrt(mse), 2)}\")\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('XGB with PCA on the test set for 45_bandgap')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAPki8q_FgJP"
   },
   "source": [
    "1. **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j9Z27xRRjQdE"
   },
   "source": [
    "assert X_train.shape[0] + X_valid.shape[0] + X_test.shape[0] == df.shape[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EHE5Ak9Bi-au"
   },
   "source": [
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_valid)\n",
    "\n",
    "mse = mean_squared_error(y_valid, y_pred)\n",
    "r2 = r2_score(y_valid, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3DH1DCjmh8Mn"
   },
   "source": [
    "plt.scatter(y_valid, y_pred)\n",
    "\n",
    "plt.ylabel('Predicted')\n",
    "plt.xlabel('Measured')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cN0cKwgvkUl5"
   },
   "source": [
    "# Random Forest 45 Hyperparameter optimisation\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    #'regressor__n_estimators': [1000],\n",
    "    #'regressor__max_features': [1.0, 'sqrt', 'log2']\n",
    "    'regressor__max_depth': [8, 10, 12],\n",
    "    'regressor__min_samples_split': [4, 6, 8],\n",
    "    'regressor__min_samples_leaf': [3, 5, 7]\n",
    "}\n",
    "\n",
    "rf = Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"regressor\", RandomForestRegressor(n_estimators=1000, max_features=1.0))\n",
    "                    ])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1,  return_train_score=True)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Retrieve the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best parameters found: {best_params}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "k80MIA9TyP3d"
   },
   "source": [
    "# Results for all parameter combinations, lookibg fo the best performance\n",
    "grid_search.cv_results_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UfmdSL9PENC"
   },
   "source": [
    "Extra trees on 45"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZW3ZgOUI6tYB"
   },
   "source": [
    "# Extra Trees 45 Hyperparameter optimisation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    #'regressor__n_estimators': [1000],\n",
    "    #'regressor__max_features': [1.0, 'sqrt', 'log2']\n",
    "    #'regressor__max_depth': [18, 23, 28],\n",
    "    #'regressor__min_samples_split': [4, 6],\n",
    "    #'regressor__min_samples_leaf': [3, 5]\n",
    "}\n",
    "\n",
    "rf = Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"regressor\", ExtraTreesRegressor(n_estimators=1000, max_features=1.0, min_samples_leaf=3, min_samples_split=4, max_depth=18))\n",
    "                    ])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1,  return_train_score=True)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Retrieve the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best parameters found: {best_params}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "RDiEiDHW7BEo"
   },
   "source": [
    "grid_search.cv_results_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrjNADmWMA2E"
   },
   "source": [
    "**Perfomance of the Extra Trees Regressor on the 45df with optimal hyperparameters on the validation set is mean_test_score: 0.699859**\n",
    "\n",
    "**Optimal hyperparameters are: n_estimators=1000, max_features=1.0, min_samples_leaf=3, min_samples_split=4, max_depth=18**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i-kCYb0UMn5C"
   },
   "source": [
    "# Do this at the end for ET\n",
    "rf = Pipeline([ (\"scaler\", StandardScaler()),\n",
    "                    (\"regressor\", ExtraTreesRegressor(n_estimators=1000, max_features=1.0, min_samples_leaf=3, min_samples_split=4, max_depth=18)) # insert your best parameters here\n",
    "                    ])\n",
    "\n",
    "rf.fit(X_train_val, y_train_val)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "vqfyRW7kNVI-"
   },
   "source": [
    "x = y_test\n",
    "y = y_pred\n",
    "\n",
    "plt.scatter(x, y, alpha=0.4, label=f\"r^2 = {round(r2, 2)}, RMSE = {round(np.sqrt(mse), 2)}\")\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('Extra Trees on the test set for 45_bandgap')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyPEY4WJPQf0"
   },
   "source": [
    "Extra Trees with PCA on 45"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rOvoNSvSPVDs"
   },
   "source": [
    "#Extra Trees with PCA 45 Hyperparameter optimisation\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "param_grid = {\n",
    "     'regressor__n_estimators': [300, 500, 1000],\n",
    "    'regressor__max_features': [1.0, 'sqrt', 'log2'],\n",
    "    'regressor__max_depth': [16, 18, 23],\n",
    "    'regressor__min_samples_split': [2, 4, 6],\n",
    "    'regressor__min_samples_leaf': [3, 5]\n",
    "}\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "et = Pipeline([(\"scaler\", StandardScaler()),\n",
    "               ('PCA', pca),\n",
    "               (\"regressor\", ExtraTreesRegressor())\n",
    "            ])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=et, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1, return_train_score=True)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Retrieve the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best parameters found: {best_params}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pFxaJyPRbHW"
   },
   "source": [
    "2. **Gaussian process regression**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DWCGbQK5zSKe"
   },
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, WhiteKernel\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    #\"regressor__kernel\": [RBF(), None],\n",
    "    # \"regressor__alpha\": [1e-5, 1e-4, 1e-3, 0.01, 0.1],\n",
    "    \"regressor__alpha\": [0.1, 1, 10],\n",
    "    #\"regressor__optimizer\": [\"fmin_l_bfgs_b\", \"fmin_ncg\"],\n",
    "    #\"regressor__n_restarts_optimizer\": [0, 1, 2]\n",
    "}\n",
    "\n",
    "# Initialize GaussianProcessRegressor\n",
    "gpr = Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"regressor\", GaussianProcessRegressor())\n",
    "                    ])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gpr, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Retrieve the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best parameters found: {best_params}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6p0sDy4UNx3g"
   },
   "source": [
    "# Results for all parameter combinations, lookibg fo the best performance\n",
    "grid_search.cv_results_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CvDaD_08M5fZ"
   },
   "source": [
    "# Do this at the end for Gaussian\n",
    "gpr = Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"regressor\", GaussianProcessRegressor(kernel=None, alpha=1e-10, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=0)) # insert your best parameters here\n",
    "                    ])\n",
    "\n",
    "gpr.fit(X_train_val, y_train_val)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdSTT9aERxv9"
   },
   "source": [
    "3. **XGB Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "olT73jenlVXl"
   },
   "source": [
    "#XGB Regressor 45 Hyperparameter optimisation\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "#from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "param_grid = {\n",
    "    \"regressor__n_estimators\":[100, 300, 500] ,\n",
    "    \"regressor__max_depth\": [2, 4, 6],\n",
    "    #\"regressor__min_samples_split\": 5,\n",
    "    \"regressor__learning_rate\": [0.1, 0.01, 0.001, 0.0001],\n",
    "    #\"regressor__loss\": \"squared_error\",\n",
    "}\n",
    "\n",
    "\n",
    "reg = Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"regressor\", XGBRegressor()) #GradientBoostingRegressor())\n",
    "                    ])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=reg, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Retrieve the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best parameters found: {best_params}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4ms46s1124S"
   },
   "source": [
    "First time run results: Best parameters found: {'regressor__learning_rate': 0.1, 'regressor__max_depth': 6, 'regressor__n_estimators': 300}"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "_RindlOssBZ3"
   },
   "source": [
    "# Results for all parameter combinations, lookibg fo the best performance\n",
    "grid_search.cv_results_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B1QwbJ0B1TPn"
   },
   "source": [
    "\n",
    "param_grid = {\n",
    "    #\"regressor__n_estimators\":[100, 300, 500] ,\n",
    "    #\"regressor__max_depth\": [6, 8, 10],\n",
    "    #\"regressor__min_samples_split\": 5,\n",
    "    \"regressor__learning_rate\": [0.5, 0.1, 0.05, 0.01],\n",
    "    #\"regressor__loss\": \"squared_error\",\n",
    "}\n",
    "\n",
    "\n",
    "reg = Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"regressor\", XGBRegressor(n_estimators=300, max_depth=8)) #GradientBoostingRegressor())\n",
    "                    ])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=reg, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Retrieve the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best parameters found: {best_params}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "pROhvKQj49Ar"
   },
   "source": [
    "# Results for all parameter combinations, lookibg fo the best performance\n",
    "grid_search.cv_results_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OWYyEY_5aOp"
   },
   "source": [
    "**Perfomance of the XGB Regressor on the 45df with optimal hyperparameters on the validation set is: 0.70288743**\n",
    "\n",
    "**Optimal hyperparameters are: n_estimators=300, max_depth=8, learning_rate=0.05**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "liqs8VhCsEun"
   },
   "source": [
    "# Do this at the end for XGB\n",
    "gpr = Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"regressor\", XGBRegressor(n_estimators=300, max_depth=8, learning_rate=0.05)) # insert your best parameters here\n",
    "                    ])\n",
    "\n",
    "gpr.fit(X_train_val, y_train_val)\n",
    "\n",
    "y_pred = gpr.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U6Oq82C3mgIh"
   },
   "source": [
    "x = y_test\n",
    "y = y_pred\n",
    "\n",
    "plt.scatter(x, y, alpha=0.4, label=f\"r^2 = {round(r2, 2)}, RMSE = {round(np.sqrt(mse), 2)}\")\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('XGBoost on the test set')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZQCPgQrr65S"
   },
   "source": [
    "4. **MLP Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JZXMYcES8CDD"
   },
   "source": [
    "#We dont need this right now, only later when modelling.\n",
    "#Joining data sets into one data frame\n",
    "\n",
    "df_120 = df_120.set_index(['MOF'])\n",
    "df_sine = df_sine.set_index(['MOF'])\n",
    "#df_qmof_refcodes = df_qmof_refcodes.set_index(['MOF'])\n",
    "df_ofm_fp = df_ofm_fp.set_index(['MOF'])\n",
    "\n",
    "\n",
    "df_joinn = df_45.join(df_120, how='outer', rsuffix='_120').join(df_sine, how='outer', rsuffix='_sine').join(df_ofm_fp, how='outer', rsuffix='ofm_fp')\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
